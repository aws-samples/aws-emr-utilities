# // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# // SPDX-License-Identifier: MIT-0
# Kyuubi server numbers
replicaCount: 2
image:
  repository: melodydocker/kyuubi-emr-eks
  tag: emr6.15_kyuubi1.8
  # pullPolicy: Always

# priorityClass used for Kyuubi server pod
priorityClass:
  create: false
  name: ~

# ServiceAccount used for Kyuubi create/list/delete pod in EMR on EKS namespace "emr"
# this SA was created by scirpts/eks_provision.sh, update the script if needed
serviceAccount:
  create: false
  name: cross-ns-kyuubi

# Allow Kyuubi create Spark driver & executor pods in a different namespace "emr" - an EMR on EKS namepsace
engine:
  namespace: emr

# the rbac role will be created in "emr" namespace  
rbac:
  create: true
  rules:
    - apiGroups: [""]
      resources: ["services", "configmaps", "serviceaccounts", "pods"]
      verbs: ["get", "list", "describe", "create", "edit", "delete", "annotate", "patch", "label", "watch"]
    - apiGroups: [""]
      resources: ["persistentvolumeclaims"]
      verbs:  ["create", "list", "delete"]
    - apiGroups: ["rbac.authorization.k8s.io"]
      resources: ["roles", "rolebindings"]
      verbs: ["get", "list", "describe", "create", "edit", "delete", "annotate", "patch", "label"]  
  
monitoring:
  prometheus:
    enabled: false

kyuubiConfDir: /usr/lib/kyuubi/conf
# kyuubiConf: {}
kyuubiConf:
  kyuubiEnv: |
    #!/usr/bin/env bash
    export KYUUBI_WORK_DIR_ROOT=/usr/lib/kyuubi/work
    export JAVA_HOME=/etc/alternatives/jre
  kyuubiDefaults: |
    kyuubi.engine.type=SPARK_SQL
    kyuubi.engine.share.level=USER
    kyuubi.session.engine.initialize.timeout=PT3M
    kyuubi.session.engine.idle.timeout=PT5M

    # # Authentication (client users)
    # kyuubi.authentication=LDAP
    # kyuubi.authentication.ldap.url=ldap://ldap-svc:389
    # kyuubi.authentication.ldap.baseDN=dc=hadoop,dc=local
    # kyuubi.authentication.ldap.domain=hadoop.local
    # kyuubi.authentication.ldap.binddn=cn=admin,dc=hadoop,dc=local
    # kyuubi.authentication.ldap.bindpw=admin

# $SPARK_CONF_DIR directory
sparkConfDir: /etc/spark/conf
sparkConf:
  sparkEnv: |
    #!/usr/bin/env bash
    export HADOOP_CONF_DIR=/etc/hadoop/conf
    export SPARK_HOME=/usr/lib/spark
    export SPARK_CONF_DIR=/etc/spark/conf
    export SPARK_LOG_DIR=/opt/spark/logs
    export JAVA_HOME=/etc/alternatives/jre
    export KYUUBI_WORK_DIR_ROOT=/usr/lib/kyuubi/work
    export SPARK_LOG_MAX_FILES=5
  sparkDefaults: |
    # Spark overrides
    ## cluster mode requires to specify a DFS file system (S3, HDFS, ..)
    ## we pass all the required jars using the spark.jars conf
    spark.submit.deployMode=cluster

    # Spark Dynamic Allocation
    spark.dynamicAllocation.enable=true
    spark.dynamicAllocation.initialExecutors=1
    spark.dynamicAllocation.maxExecutors=10
    spark.dynamicAllocation.executorIdleTimeout=2min
    spark.dynamicAllocation.cachedExecutorIdleTimeout=30min
    # shuffle tracking is required in k8s unless using RSS
    spark.dynamicAllocation.shuffleTracking.enabled=true
    spark.dynamicAllocation.shuffleTracking.timeout=30min
    spark.cleaner.periodicGC.interval=5min

    # EKS settings
    spark.kubernetes.namespace=emr
    # Kyuubi's service account that has permisison to access the EMR's namespace
    spark.kubernetes.authenticate.serviceAccountName=cross-ns-kyuubi
    # SA in EMR namespace
    spark.kubernetes.authenticate.driver.serviceAccountName=emr-kyuubi
    spark.kubernetes.container.image=melodydocker/kyuubi-emr-eks:emr6.15_kyuubi1.8
    spark.kubernetes.executor.podNamePrefix=kyuubi
    spark.kubernetes.driver.podTemplateContainerName=spark-kubernetes-driver
    spark.kubernetes.executor.podTemplateContainerName=spark-kubernetes-executor
    spark.kubernetes.driver.podTemplateFile=/etc/spark/conf/driver-template.yaml
    spark.kubernetes.executor.podTemplateFile=/etc/spark/conf/executor-template.yaml
    spark.kubernetes.file.upload.path=s3://emr-on-eks-rss-021732063925-us-west-2
    spark.master=k8s://https://kubernetes.default.svc:443
    
    # S3 settings
    spark.hadoop.fs.s3.impl=com.amazon.ws.emr.hadoop.fs.EmrFileSystem
    spark.hadoop.fs.s3.customAWSCredentialsProvider=com.amazonaws.auth.WebIdentityTokenCredentialsProvider
    spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native
    spark.hadoop.fs.AbstractFileSystem.s3.impl=org.apache.hadoop.fs.s3.EMRFSDelegate
    spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds=2000
    spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem=2
    spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem=true
    
    # Hive Data Catalog (Glue or External)
    spark.hadoop.hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
    
    # Hudi, Delta, Iceberg configs
    spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    # org.apache.kyuubi.plugin.spark.authz.ranger.RangerSparkExtension
    spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
    spark.jars=local:///usr/lib/hudi/hudi-spark-bundle.jar,local:///usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar,local:///usr/share/aws/delta/lib/delta-core.jar,local:///usr/share/aws/delta/lib/delta-storage.jar
# spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.iceberg.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog
# spark.sql.catalog.iceberg.warehouse=s3://YOUR_BUCKET/iceberg/
# spark.sql.catalog.iceberg.io-impl=org.apache.iceberg.aws.s3.S3FileIO
# spark.hadoop.fs.s3a.bucket.iceberg-bucket.committer.magic.enabled=true
# spark.hadoop.fs.s3a.bucket.YOUR_BUCKET.committer.magic.enabled=true
  log4j2: |
    rootLogger.level=debug
    rootLogger.appenderRef.stdout.ref=STDOUT
    # Console Appender
    appender.console.type=Console
    appender.console.name=STDOUT
    appender.console.target=SYSTEM_OUT
    appender.console.layout.type=PatternLayout
    appender.console.layout.pattern=%d{HH:mm:ss.SSS} %p %c: %m%n
    appender.console.filter.1.type=Filters
    appender.console.filter.1.a.type=ThresholdFilter
    appender.console.filter.1.a.level=info
    # Set the default kyuubi-ctl log level to WARN. When running the kyuubi-ctl, the
    # log level for this class is used to overwrite the root logger's log level.
    logger.ctl.name=org.apache.kyuubi.ctl.ServiceControlCli
    logger.ctl.level=error
    # Kyuubi BeeLine
    logger.beeline.name = org.apache.hive.beeline.KyuubiBeeLine
    logger.beeline.level = error
